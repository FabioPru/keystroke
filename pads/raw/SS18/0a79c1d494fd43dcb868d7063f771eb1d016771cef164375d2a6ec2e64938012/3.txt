Learning Journal 3
The lecture for learning journal 3 was a follow-up for the previous lecture on Research Methods in Linguistics and contained four parts, namely the term "quantitive turn in linguistics", general terminology, the three crucial desiderata of quantitative research and the various types of variables based on scales of measurements.
The quantitative turn in linguistics describes a shift in linguistic research from Structuralism and Generativism approaches to usage-based and experience-driven analysis. In the 20th century, the ideas of Noam Chomsky, founder of structuralistic linguistic theories, dominated the linguistic research. He practised a kind of Armchair linguistics, where all evidence for linguistic phenomena was self-invented. This was possible because language was thought to be strictly systematic and invariable in its clear-cut structure. Nowadays, the importance of statistical data for finding patterns is stressed in all linguistic fields, since language was observed to be not at all invariable. Instead, language is now seen as highly individual for every user and therefore definitely variable. This variability can only be studied through quantitative data.
In order to collect this data, you need conduct comparable and relicable research, for which the following terms are key. The population is the group that you want to observe to gain representative data for your research. The parameters are then those representative characteristics. Since it's impossible to use the entire population for your research due to the sheer number of individuals, we usually use samples as a representation of the whole population. But then again, because this representation is just an assumption, the data of the sample group differs from the data of the entire population most of the time. This is what we call a sampling error. The smaller it is, the better does the sample represent the whole population. There are three ways to determine which member of the population is used in the sample group. The sampling can either be done randomly, so that every member of the population has an equal chance of being part of the sample group, representatively, so that the members of the sample group are selected based on certain characteristics or simply based on convenience, so that every easily available member of the population is used.
Especially in linguistics, it can sometimes be problematic to naturally measure the matter of examination in some way. Operationalisation is the process of turning a normally not measurable phenomenon into a concept to make it susceptible to empirical observations. If the hypothesis of the research is based on such operationalisation, we call it a statistical hypothesis. If it is, on the other hand, a prediction of natural language, we speak of a text hypothesis. Statistics in general is a term to describe certain techniques and tools in order to analyse data. More spefically, it is a way to refer to whatever measure we gain from the observed samples.
Descriptive statistics is a mere description of the observed characteristics of a sample. In order to say something about a central tendency in descriptive statistics, we use the following values: Mode, which is the most frequently occuring value in the data set. Median, the value in the middle of all ordered values. And mean, the sum of measurements divided by the number of measurements. Measures of frequency indicate how often a particular feature occurs and is also a mean of visualising trends in the data and determining the appropriateness of the data.
In order to make a statement about the variability in descriptive statistics, we use the following values: Range, the difference between the highest and lowest value. Interquartile range, the same but with quarter steps. Variance, the average squared difference of the scores from the mean, which tells us, how close the scores in the distribution are to the middle of the distribution. And standard deviation, the square root of the variance, which tells us, how far the scores of the distribution diverge from the middle on average. while inferential statistics goes one step further and uses those characteristics to actually make a prediction about what the whole population is like in terms of this characteristic based on the observed sample under a certain degree of confidence to which the sample is indeed representative. This is done using the test statistics to determine the probability of falsely rejecting the null hypothesis (p-value).

The are three different requirements or desiderata of quantitative research.
For one, we have validity, which can be divided into internal and external validity. Internal validity describes the extent to which the result of the research is warrant, because it is not in any way biased by other factors besides the desired ones in the study. A threat to internal validity might by for example that the participants score increasingly better in a test, because of their experience from previous tests. This might then indicate that the experiment is poorly designed. External validity describes the extent to which the result of the research is warrant and can be applied to other contexts and therefore generalised. Again, this depends on whether the results are biased by any factors specific to the observed sample. Appropriate operationalisation is one way to eliminate bias and increase the validity. Other ways would be to have a very large sample group and/or to design the study in such a way that only the aspects relevant for your research influence the data.
Reliability describes whether the measurements of a study produce similar results under consistent conditions either by looking at the system of measurement (inter-rater reliability) or the means of measurement (instrument reliability). Reliability does not imply validity, because despite consistency in measurement, the study might still not be representative for what you want to observe due to some kind of bias. It is a part of validity, however. Trying to measure the amount of passive constructions in a text by looking at "be"-constructions only would, for example, not be a reliable way of measuring, because "be" does not only occur in passive constructions, but also in active constructions.
Lastly, replicability is a concept to describe if and in how far a similar research conclusion can be achieved if the study is independently replicate under different conditions. For example, if a study suggests that professional British writers use the passive more often than non-professional British writers and we conclude that with greater writing proficiency, the use of the passive voice increases, we would have to have a similar research finding when examining the same for American writers.

In the last lecture, we already discussed the difference between dependent and independent variables. Another fundamental division can be made between continuous and categorical variables. Continuous variables can take on any value in a certain range, like the amount of passive constructions used in a text, while categorical variables can only take on a pretermined number of values and therefore fall into specific categories. An example for this would be the general division between whether a person uses more passive or active constructions in a text. Since this discrimination is binary, we also call this a binary variable. Apart from that, variables can too be dinstinguished by their scales of measurement: Nominal variables take on mutually exclusive states/categories, e.g. whether a person uses more passive or active constructions in a text. Ordinal variables take on states that can be structered by a rank order e.g. when measuring opinion, answers that range from "completely agree" to "completely disagree". Interval variables take on states on a constant scale of intervals, e.g. a date measured from an arbitrary point in time. Ratio variables also take on states on a constant scale of intervals, but this scale also includes the state "zero", which is unique and non-arbitrary, e.g. the Kelvin temperature scale.
