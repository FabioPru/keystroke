Learning Journal 10

In contrast to last week, this week's lecture did not focus on syntactic complexity but on lexical complexity. Like syntactic complexity, lexical complexity is also a part of the Complexity-Accuracy-Fluency (CAF) triad and a measure for the complexity dimension in particular. Lexical complexity is a cover-term that is synonymous to lexical diversity and lexical richness. Research has shown that lexical complexity and the ensuing larger vocabulary knowledge is strongly related to later academic achievements as well as vocational and social success. It can, furthermore, be linked to predicting reading comprehension ability for native speakers and second language learners alike and is a key scoring criterion for a variety of tests and descriptors of proficiency levels. Traditionally, lexical complexity has been assessed using tests that fit into the four categories "Multiple choice tests", "Completions tests", "Translation tests" and "Matching tasks". Nowadays, however, these assessment types have slowly faded from the spotlight and there has been a rising interest in using natural, extended, free-style language production instead.
Lexical complexity is a multidimensional concept which, in broader terms, describes the range and degree of sophistication of an individual's productive vocabulary and is concerned with the features lexical density, lexical variation, lexical sophistication and number of overall errors in particular. While the latter is usually only relevant for lower proficiency levels since errors occur a lot less frequently the more proficient in a language someone is, the other factors all play a huge role as indicators for lexical complexity. Lexical density is operationalised as the proportion of lexical words in comparison with the number of grammatical words. Lexical variation is concerned with the type-token-ratio, i.e. vocabulary size measured as the range of lexical words used, and lexical sophistication describes the extent to which an individual uses more unusual and advanced words instead of more common ones. The notion of advanced and common words are differentiated using Zipf's law which describes how the words in a language can be ordered according to their frequency and how the most frequent word tends to be about twice as frequent as the second most frequent and about three times as frequent as the third most frequent one and so on. It must be noted, though, that lexical sophistication is not characterised by advanced words alone. Instead, Zipf himself gave a psychological reasoning for his discovered phenomenon and argued that humans naturally strive for an optimal balance between redundancy and sophistication when producing language. What this means is that there has to be a certain number of more common words in productive language use or else the listener or reader will become less attentive over time. This factor has to be taken into consideration when assessing lexical sophistication.
As before, automatic tools like the lexical complexity analyzer and TAALES facilitate the assessment process and learner corpora, as large collections of quasi-naturally produced data compiled according to explicit design criteria, are a suitable fit for lexical complexity investigations for L2 language use.
The following case study by Jarvis illustrates how automatic assessment tools and learner corpora can be used to compare automatic assessments with human assessments in order to see if and in how far lexical diversity affects human judgements when it comes to language proficiency assessment. This means, he looked at the inter-rater reliability, i.e. whether the evaluations of the data are consistent for different human raters. This can either be determined by simply looking at the mere percentage agreement, where above 75% would commonly considered "good" and anything above 90% "ideal", or by using Cohen's Kappa, which looks at the average rate of agreement and where a score between 0.81 and 1.00 is considered "excellent". Other methods that should be mentioned but are not further explained are Pearson's product moment and the Spearman rank correlation coefficients. Since Jarvis was interested in human perception, his study follows an emic approach. The term emic is derived from the word phonemics, which is the study of the human perception of sounds and, therefore, a subjective approach, as opposed to etic, which comes from phonetics, the objective study of all existing sounds. His corpus data was compromised by 276 written narrative retells of an eight-minute segment of a silent Charlie Chaplin film all written in English. 140 narratives came from learners with a Finnish L1 background in grades 5, 7 and 9; 70 narratives came from learners with a Swedish L1 background in grades 7 and 9; and 66 narratives came from native English speakers in grades 5, 7 and 9. All of them were rated according to a 26-point scale by 11, 20, 21 and 20 raters in the years 2011, 2012, 2014 and 2015 respectively, since it was found that a minimum of 20 raters is necessary for meaningful results.
Jarvis found out that even untrained people, i.e. people who were not explicitly told to look for lexical diversity, intuitively took this dimension into account when asked to assess the language proficiency in the texts. However, the more trained they were, the higher the inter-rater reliability was. His bottom-line conclusion was a model he proposed for developing a corpus-specific automated measure of lexical diversity which thinks of emic approaches as a necessary part for meaningful correlational analyses. According to him, the first development step is to let a sub-corpus be evaluated by human raters. And then, based on their judgement, the construct should be defined and an objective operationalisation should be determined in order to develop a statistical model on the basis of which the automated measure is chosen and the final learner corpus results from.
