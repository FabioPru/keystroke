Learning Journal 9

Lecture 10 of Corpus Linguistics dealt with the assessment of L2 writing using the Complexity-Accuracy-Fluency triad and took a closer look at the measure of complexity in particular.
The Complexity-Accuracy-Fluency (CAF) triad is one of the major conceptual framworks for L2 assessment and especially used for comparing language proficiency levels using language development benchmarks based on the three dimensions complexity, accuracy and fluency. Linguistic complexity is a dimension that is expressed on different levels, e.g. morphologically, regarding the lexicon or concerning syntax.
This week's lecture focused on syntactic complexity, which is mainly characterised by the amount of variation in the syntactic structures and their sophistication. Due to its crucial significance for proficient writing, it has become a vital part of many international writing assessment tests and reference frameworks including the ECPE (Examination for the Certificate of Proficiency in English), CEFR (Common European Framwork of Reference for Languages) or the TOEFL (Test of English as a Foreign Language) and has also been included in many theoretical models of L2 writing used for L2 testing.
The problem is that the scoring criteria and descriptors of the various proficiency levels are usually very abstract. In order to make them more tangible, they can be operationalised using measures that fit into the following general categories: Measures of the length of the clause or sentence, measures determining the types and number of occurences of embedding (e.g. "I like going to the cinema."), measures of the types and incidences of coordination (and, but, or etc.) and measures that quantify/operationalise the range and types of phrasal units including their relative frequency.
An example for a measure of the length of the clauses/sentences is the T-unit. A T-unit is defined as the main clause plus all associated dependent clauses. For the mean length of the T-units, the overall number of words per T-unit is counted and averaged across all T-units in the text.
Another exemplary measure is the noun phrase complexity. Long noun phrases are a typical feature in texts produced by highly proficient writers and can, therefore, be found the most in academic papers or publications. It is one of the most obvious and noticable signs of complexity for most people, because with an increasing number of modifiers per noun phrase, the sentences tend to get harder to understand for inexperienced listeners who are unaccustomed to this particular style of writing.
The normed rate of occurences of non-finite relative clauses like in "the method used here" is a third example for a measure of complexity. It fits into the category of measures that look at the range and types of phrasal units and their frequency.
Of course, these measure do not have to be calculated by hand. Instead, automatic computational analysis tools like Biber Tagger, COH-Metrix, L2 Syntactic Complexity Analyzer or TAASSC are used for the assessment. While this facilitates the actual process of assessment, which would otherwise be an enormous task to do by hand, the computational softwares do not undertake every task. Choosing the appropriate measure of complexity for the assignment observed is still up to the researcher and an important responsibility for that the data could otherwise be useless and inconclusive. The following example illustrates this quite clearly. The main assumption in L2 writing assessment is that the syntactic complexity continually increases with the degree of proficiency of the writer. Traditionally, it was thought that the mean length of the T-units is a universally appropriate tool for assessing the writing development, so researchers relied on it. However, after further investigation, it was shown that casual conversation and academic writing are identical in T-unit length which would mean that both are equally syntactically complex, which is objectively not the case. Where they do diverge, on the other hand, is the overall number of embedded dependent clauses, which is usually a lot higher in conversation than in academic writing. This, again, would mean that academic writing is superior to conversation concerning syntactic complexity. It turns out that  academic writing is usually distinguished by long and complex noun phrases rather than long T-lengths or a high degree of embedding. Consequently, the reliance on the elaborated structures has been challenged; the intended assessment measures should be critically evaluated with regards to their appropriateness for every new project.
But that's not the only thing automatic assessment has given us insight into. With the help of computational tools and elicited data from learner corpora and by comparing these results with how humans assess language proficiency based on syntactic complexity, it was found that these theoretical syntactic complexity measures actually correlate with the measures human raters based their L2 writing quality scores on. This means that they are indeed a good representation and operationalisation of perceived language proficiency and, therefore, even more valuable in terms of assessments and theory on targeted writing improvements.
