  

Linguistic complexity, as a part of the CAF triad, is a multi-dimensional feature to observe and classify written and spoken language. After concentrating on syntactic complexity while the last lecture, now another part of linguistic complexity, the lexical complexity, is the next feature to deal with. Lexical complexity covers aspects like lexical richness or lexical diversity. 
These features are also important for second language acquisition because with the help of analysing the lexical complexity it is possible to make assumptions about the learner´s vocabulary size and even predict their social and vocational success. Due to the studies that identified these connections between lexical richness and academic and social outcomes a first step was made to even use lexical complexity as a significant predictor for reading comprehension ability of second language learners and even native speakers. Therefore lexical diversity is also used as a criteria for several tests to describe and classify proficiency levels as in the CEFR (Common European Framework of Reference for Languages). 
Especially traditional vocabulary tests have been used for a long time to assess the lexical richness of learners; with the help of multiple choice tests, completion tests, translation tests and matching tests. Nowadays there is an increase of researches which are  focussing on naturally occuring vocabulary in a more freely surrounding. 
Lexical diversity is, as well as linguistic complexity, a multi-dimensional feauture. It includes four components to describe a L2 learner´s language use: lexical density, lexical variation, lexical sophistication and the number of errors in vocabulary use. Lexical density is dealing with the relation between grammatical and lexical words in a text, lexical variation describes to what extent people use different words (analysing the type-token-ratio), lexical sophistication is defined as the number or percentage of advanced vocabulary used in the text, errors in vocabulary use is focusing on the mistakes a text producer makes. 
Two tools to automatically assess lexical diversity are the Lexical Complexity Analyzer by Lu and Taales by Kyle. 
A linguist who ran a study on learner corpora and lexical diversity is Scott Jarvis. To present that study it is important to distinguish and clarify between an emic and etic approach to lexical diversity. Etic approaches focus on automatically and computational measures to assess lexical complexity. Emic approaches though also have a look on the `insider views`, for example compare if there are differences or similarities between the results of computational measures and the results of a reliable set of human raters. 
The main goal of the study in 2017 was to determine whether and to what extent lexical diversity is a genuine emic phenomenon or foster high-levels of inter-rater reliabilty. The data exists out of 276 retells of an eight-minute silent Charlie Chaplin film segment. 140 of these narrative retells were written by Finish-speaking learners of English in grades 5, 7 and 9, 70 narrative retells were written by swedish-speaking learners of English in grades 7 and 9 and 66 retells were written by native Engish speaker in grades 5, 7 and 9. That corpus has been rated for writing quality by two experienced raters on a scale of 0-8. The inter-rater reliability was found to be 0.94. That shows that the raters judged and evaluated the same set of data in a highly significant similar way. The calculating of the inter-rater reliabilty assures that a part of realiability as a component of a quantitive research is fullfilled. (The other two are validity and replicability.) 
The study proves that lexical deviation is a matter of perception and is not only based on specific computational measurable features. Furthermore it proves that competent speakers of a common language share similar perceptions with regard to lexical deviation. 
